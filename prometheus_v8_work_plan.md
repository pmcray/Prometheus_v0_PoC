
# Project Prometheus v0.8 Work Plan: The Collaborative Scientist

**Generated by Gemini**

---

## Abstract

This document outlines the detailed work plan for the next major phase of the Project Prometheus demonstrator, v0.8. This phase, titled "The Collaborative Scientist," focuses on the critical and often overlooked aspects of advanced AI systems: their ability to interact with humans, manage resources, and provide transparency into their complex reasoning processes. This phase directly addresses the strategic considerations of the "Human-in-the-Loop Bottleneck" and "Compute Scalability" (Section 4.1), while also making the system's intelligence more observable and auditable.

---

## Task 1: Implement Scalable Oversight - AI-Assisted Auditing

### Rationale

As the agent's speed of thought and action increases, human oversight becomes a bottleneck. The solution, as outlined in Section 4.1, is "scalable oversight," where the AI itself is used to help supervise the AI. This task implements a basic version of this by having the agent summarize its own complex actions for a human reviewer.

### Methodology

1.  **New `AuditTool`:** A new tool will be created that can take a complex data structure (like a `ProofTree` or a sequence of experimental results) and generate a high-level, natural-language summary.
2.  **New `AuditorAgent`:** A new, specialized agent will be created. Its sole purpose is to observe the main `MCSSupervisor` loop. After a complex task (like a proof search or an experimental cycle) is complete, the `AuditorAgent` will be given the final results and the `AuditTool`.
3.  **Generate Audit Trail:** The `AuditorAgent` will use the `AuditTool` to generate a concise summary of the primary agent's actions, highlighting key decisions, failures, and the final outcome. This summary is the "audit trail" for the human supervisor.

### Deliverables

-   An `AuditTool` for summarizing complex data structures.
-   A new `AuditorAgent` responsible for observing and summarizing the system's actions.
-   A clear, human-readable audit trail logged at the end of each major task.

### Verification

After a multi-step proof search, the `AuditorAgent` must successfully generate a summary that accurately describes the key steps of the proof, including any major branches that were explored and abandoned.

---

## Task 2: Implement Resource Self-Management

### Rationale

The "Compute Scalability" section (4.1) notes that an advanced AI must be incentivized to treat its own computational resources as a key performance metric. This task gives the agent a "budget" and forces it to optimize its own resource usage.

### Methodology

1.  **Introduce `compute_budget`:** The `MCSSupervisor` will be given a `compute_budget` (e.g., a set number of LLM calls).
2.  **Track Resource Usage:** The `CoderAgent` and `PlannerAgent` will be modified to track the number of LLM calls they make.
3.  **New `PlannerAgent` Goal:** The `PlannerAgent` will be given a new, high-level goal: "Solve the given problem *within the specified compute budget*."
4.  **Budget-Aware Reasoning:** The `PlannerAgent`'s prompts will be updated to include the remaining compute budget. This will force it to make more strategic decisions, such as choosing a less computationally expensive but potentially less direct proof strategy if the budget is low.

### Deliverables

-   A `compute_budget` mechanism in the `MCSSupervisor`.
-   Resource tracking in the `CoderAgent` and `PlannerAgent`.
-   An updated `PlannerAgent` that can reason about and optimize its own resource usage.

### Verification

When given a complex task and a tight compute budget, the agent must demonstrably alter its strategy to conserve resources. For example, it might choose a 3-tactic proof instead of a 5-tactic proof, even if the 5-tactic proof was its first choice, because the 3-tactic proof is "cheaper."

---

## Task 3: Human-in-the-Loop - Interactive Goal Setting

### Rationale

This task directly addresses the "Human-in-the-Loop Bottleneck" by creating a more collaborative relationship between the human operator and the AI. Instead of a static, pre-defined goal, the human can now interact with the agent to refine and clarify the objective.

### Methodology

1.  **Interactive `main.py`:** The main script will be modified to be interactive. It will prompt the user to choose a task (e.g., "prove a theorem" or "run an experiment").
2.  **Clarification Dialogue in `PlannerAgent`:** The `PlannerAgent` will be upgraded. If it receives an ambiguous goal from the user (e.g., "solve a math problem"), it will be able to ask clarifying questions (e.g., "What is the specific theorem you would like me to prove?").
3.  **Human Feedback Loop:** The `MCSSupervisor` will be modified to handle this interactive dialogue, passing the user's responses back to the `PlannerAgent` until the goal is sufficiently well-defined to be executed.

### Deliverables

-   An interactive `main.py` script for goal selection.
-   A `PlannerAgent` capable of asking clarifying questions to resolve ambiguity.
-   A `MCSSupervisor` that can manage an interactive, human-in-the-loop dialogue for goal setting.

### Verification

When the user provides an ambiguous goal like "Analyze this paper," the system must respond with a clarifying question like "What specific information are you looking for in the paper?" The user's response should then be used to form a precise, executable goal.
